# -*- coding: utf-8 -*-
# from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf

from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops, embedding_ops, math_ops, nn_ops  #, rnn_cell
from tensorflow.models.rnn import rnn_cell
from tensorflow.python.ops import variable_scope as vs

flags = tf.app.flags
FLAGS = flags.FLAGS

VINYALS_KAISER = 'vinyals_kayser'
LUONG_GENERAL = 'luong_general'
LUONG_DOT = 'luong_dot'


def embedding_attention_decoder(decoder_inputs, initial_state, attention_states, cell,
                                batch_size, num_symbols, window_size=10,
                                output_size=None, output_projection=None, input_feeding=False,
                                feed_previous=False, attention_type=None, content_function=VINYALS_KAISER,
                                dtype=tf.float32, scope=None):
    """RNN decoder with embedding and attention and a pure-decoding option.

    Args:
      decoder_inputs: a list of 1D batch-sized int32 Tensors (decoder inputs).
      initial_state: 2D Tensor [batch_size x cell.state_size].
      attention_states: 3D Tensor [batch_size x attn_length x attn_size].
      cell: rnn_cell.RNNCell defining the cell function.
      batch_size:
      num_symbols: integer, how many symbols come into the embedding.
      num_heads: number of attention heads that read from attention_states.
      output_size: size of the output vectors; if None, use cell.output_size.
      output_projection: None or a pair (W, B) of output projection weights and
        biases; W has shape [output_size x num_symbols] and B has shape
        [num_symbols]; if provided and feed_previous=True, each fed previous
        output will first be multiplied by W and added B.
      feed_previous: Boolean; if True, only the first of decoder_inputs will be
        used (the "GO" symbol), and all other decoder inputs will be generated by:
          next = embedding_lookup(embedding, argmax(previous_output)),
        In effect, this implements a greedy decoder. It can also be used
        during training to emulate http://arxiv.org/pdf/1506.03099v2.pdf.
        If False, decoder_inputs are used as given (the standard decoder case).
      attention_type:
      dtype: The dtype to use for the RNN initial states (default: tf.float32).
      scope: VariableScope for the created subgraph; defaults to
        "embedding_attention_decoder".

    Returns:
      outputs: A list of the same length as decoder_inputs of 2D Tensors with
        shape [batch_size x output_size] containing the generated outputs.
      states: The state of each decoder cell in each time-step. This is a list
        with length len(decoder_inputs) -- one item for each time-step.
        Each item is a 2D Tensor of shape [batch_size x cell.state_size].

    Raises:
      ValueError: when output_projection has the wrong shape.
    """

    assert attention_type is not None

    if output_size is None:
        output_size = cell.output_size
    if output_projection is not None:
        proj_weights = ops.convert_to_tensor(output_projection[0], dtype=dtype)
        proj_weights.get_shape().assert_is_compatible_with([cell.output_size,
                                                            num_symbols])
        proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)
        proj_biases.get_shape().assert_is_compatible_with([num_symbols])

    with vs.variable_scope(scope or "embedding_attention_decoder"):
        with ops.device("/cpu:0"):
            if input_feeding:
                embedding = vs.get_variable("embedding", [num_symbols, cell.input_size / 2])
            else:
                embedding = vs.get_variable("embedding", [num_symbols, cell.input_size])

        def extract_argmax_and_embed(prev, _):
            """Loop_function that extracts the symbol from prev and embeds it."""
            if output_projection is not None:
                prev = nn_ops.xw_plus_b(
                        prev, output_projection[0], output_projection[1])
            prev = array_ops.stop_gradient(math_ops.argmax(prev, 1))
            # x, prev_symbol = nn_ops.top_k(prev, 12)
            prev_symbol = array_ops.stop_gradient(prev)
            emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)
            return emb_prev

        loop_function = None
        if feed_previous:
            loop_function = extract_argmax_and_embed

        emb_inp = [
            embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]

        return _attention_decoder(
                    emb_inp, initial_state, attention_states, cell, batch_size, attention_type=attention_type,
                    output_size=output_size, loop_function=loop_function, window_size=window_size,
                    input_feeding=input_feeding, content_function=content_function)


def _attention_decoder(decoder_inputs, initial_state, attention_states, cell, batch_size, attention_type=False,
                       output_size=None, loop_function=None, window_size=10, input_feeding=False,
                       content_function=VINYALS_KAISER, dtype=tf.float32, scope=None):
    """RNN decoder with local attention for the sequence-to-sequence model.

    Args:
      decoder_inputs: a list of 2D Tensors [batch_size x cell.input_size].
      initial_state: 2D Tensor [batch_size x cell.state_size].
      attention_states: 3D Tensor [batch_size x attn_length x attn_size].
      cell: rnn_cell.RNNCell defining the cell function and size.
      batch_size:
      output_size: size of the output vectors; if None, we use cell.output_size.
      num_heads: number of attention heads that read from attention_states.
      loop_function: if not None, this function will be applied to i-th output
        in order to generate i+1-th input, and decoder_inputs will be ignored,
        except for the first element ("GO" symbol). This can be used for decoding,
        but also for training to emulate http://arxiv.org/pdf/1506.03099v2.pdf.
        Signature -- loop_function(prev, i) = next
          * prev is a 2D Tensor of shape [batch_size x cell.output_size],
          * i is an integer, the step number (when advanced control is needed),
          * next is a 2D Tensor of shape [batch_size x cell.input_size].
      dtype: The dtype to use for the RNN initial state (default: tf.float32).
      scope: VariableScope for the created subgraph; default: "attention_decoder".

    Returns:
      outputs: A list of the same length as decoder_inputs of 2D Tensors of shape
        [batch_size x output_size]. These represent the generated outputs.
        Output i is computed from input i (which is either i-th decoder_inputs or
        loop_function(output {i-1}, i)) as follows. First, we run the cell
        on a combination of the input and previous attention masks:
          cell_output, new_state = cell(linear(input, prev_attn), prev_state).
        Then, we calculate new attention masks:
          new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))
        and then we calculate the output:
          output = linear(cell_output, new_attn).
      states: The state of each decoder cell in each time-step. This is a list
        with length len(decoder_inputs) -- one item for each time-step.
        Each item is a 2D Tensor of shape [batch_size x cell.state_size].

    Raises:
      ValueError: when num_heads is not positive, there are no inputs, or shapes
        of attention_states are not set.
    """

    if output_size is None:
        output_size = cell.output_size

    with vs.variable_scope(scope or "attention_decoder"):

        batch = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.
        attn_length = attention_states.get_shape()[1].value
        attn_size = attention_states.get_shape()[2].value

        # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.
        hidden = array_ops.reshape(
                attention_states, [-1, attn_length, 1, attn_size])

        attention_vec_size = attn_size  # Size of query vectors for attention.

        va = None
        hidden_features = None

        if content_function is not LUONG_DOT:

            # for a in xrange(num_heads):
            # here we calculate the W_a * s_i-1 (W1 * h_1) part of the attention alignment
            k = vs.get_variable("AttnW_%d" % 0, [1, 1, attn_size, attention_vec_size])
            hidden_features = nn_ops.conv2d(hidden, k, [1, 1, 1, 1], "SAME")
            va = vs.get_variable("AttnV_%d" % 0, [attention_vec_size])

        else:
            hidden_features = hidden

        states = [initial_state]

        outputs = []
        prev = None
        batch_attn_size = array_ops.pack([batch, attn_size])

        # initial attention state
        ctx = array_ops.zeros(batch_attn_size, dtype=dtype)
        ctx.set_shape([None, attn_size])

        for i in xrange(len(decoder_inputs)):
            if i > 0:
                vs.get_variable_scope().reuse_variables()

            if input_feeding:
                # if using input_feeding, concatenate previous attention with input to layers
                inp = array_ops.concat(1, [decoder_inputs[i], ctx])
            else:
                inp = decoder_inputs[i]

            # If loop_function is set, we use it instead of decoder_inputs.
            if loop_function is not None and prev is not None:
                with vs.variable_scope("loop_function", reuse=True):
                    inp = array_ops.stop_gradient(loop_function(prev, 12))

            # Merge input and previous attentions into one vector of the right size.
            x = rnn_cell.linear([inp] + [ctx], cell.input_size, True)

            # Run the RNN.
            cell_output, new_state = cell(x, states[-1])
            states.append(new_state)  # new_state = dt#

            dt = new_state

            last_layer_output = None
            if content_function is LUONG_GENERAL or content_function is LUONG_DOT:
                last_layer_output = cell_output
                # _, _, _, h4 = tf.split(1, 4, dt)
                # _, state = tf.split(1, 2, h4)
                # last_layer_output = state
                # last_layer_output = dt

            # Run the attention mechanism.
            if attention_type is 'local':
                ctx = _local_attention(decoder_hidden_state=dt, last_layer_output=last_layer_output,
                                       hidden_features=hidden_features, va=va, hidden_attn=hidden,
                                       attention_vec_size=attention_vec_size, attn_length=attn_length,
                                       attn_size=attn_size, batch_size=batch_size, content_function=content_function,
                                       window_size=window_size, dtype=dtype)

            elif attention_type is 'global':
                ctx = _global_attention(decoder_hidden_state=dt, last_layer_output=last_layer_output,
                                        hidden_features=hidden_features, v=va, hidden_attn=hidden,
                                        attention_vec_size=attention_vec_size, attn_length=attn_length,
                                        content_function=content_function, attn_size=attn_size)

            else:  # here we choose the hybrid mechanism
                ctx = _hybrid_attention(decoder_hidden_state=dt, last_layer_output=last_layer_output,
                                        hidden_features=hidden_features, va=va, hidden_attn=hidden,
                                        attention_vec_size=attention_vec_size, attn_length=attn_length,
                                        attn_size=attn_size, batch_size=batch_size,
                                        content_function=content_function, window_size=window_size, dtype=dtype)

            if content_function is LUONG_GENERAL or content_function is LUONG_DOT:

                with vs.variable_scope("AttnOutputProjectionLuong"):

                    ctx = tf.concat(1, [ctx, last_layer_output])

                    ctx = rnn_cell.linear([ctx], output_size, True)

                    ctx = tf.tanh(ctx)

            with vs.variable_scope("AttnOutputProjection"):
                output = rnn_cell.linear([cell_output] + [ctx], output_size, True)

            if loop_function is not None:
                # We do not propagate gradients over the loop function.
                prev = array_ops.stop_gradient(output)

            outputs.append(output)

    return outputs, states


def _hybrid_attention(decoder_hidden_state, hidden_features, va, hidden_attn, attention_vec_size,
                      attn_length, attn_size, batch_size, window_size=10, content_function=VINYALS_KAISER,
                      last_layer_output=None, dtype=tf.float32):

    local_attn = _local_attention(decoder_hidden_state=decoder_hidden_state,
                                  hidden_features=hidden_features, va=va, hidden_attn=hidden_attn,
                                  attention_vec_size=attention_vec_size, attn_length=attn_length,
                                  attn_size=attn_size, batch_size=batch_size, content_function=content_function,
                                  window_size=window_size, last_layer_output=last_layer_output, dtype=dtype)

    global_attn = _global_attention(decoder_hidden_state=decoder_hidden_state,
                                    hidden_features=hidden_features, v=va, hidden_attn=hidden_attn,
                                    attention_vec_size=attention_vec_size, attn_length=attn_length,
                                    content_function=content_function, attn_size=attn_size,
                                    last_layer_output=last_layer_output, )

    with vs.variable_scope("FeedbackGate_%d" % 0):
        y = rnn_cell.linear(decoder_hidden_state, attention_vec_size, True)
        y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])

        vb = vs.get_variable("FeedbackVb_%d" % 0, [attention_vec_size])

        # tanh(Wp*ht)
        tanh = math_ops.tanh(y)
        beta = math_ops.sigmoid(math_ops.reduce_sum((vb * tanh), [2, 3]))

        attns = beta * global_attn + (1 - beta) * local_attn

    return attns


def _global_attention(decoder_hidden_state, hidden_features, v, hidden_attn, attention_vec_size, attn_length,
                      attn_size, content_function=VINYALS_KAISER, last_layer_output=None):
    """Put attention masks on hidden using hidden_features and query."""

    with vs.variable_scope("Attention_%d" % 0):

        # a = None

        if content_function is LUONG_DOT:

            assert last_layer_output is not None

            s = math_ops.reduce_sum((hidden_features * last_layer_output), [2, 3])  # hidden features are h_s

            # a = tf.matmul(last_layer_output, hidden_features)

        elif content_function is LUONG_GENERAL:

            assert last_layer_output is not None

            s = math_ops.reduce_sum((last_layer_output * hidden_features), [2, 3])  # hidden features are Wa*h_s

        else:

            y = rnn_cell.linear(decoder_hidden_state, attention_vec_size, True)
            y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])

            # Attention mask is a softmax of v^T * tanh(...).
            s = math_ops.reduce_sum(v * math_ops.tanh(hidden_features + y), [2, 3])

        a = nn_ops.softmax(s)

        # Now calculate the attention-weighted vector d.
        d = math_ops.reduce_sum(
                array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden_attn,
                [1, 2])
        ds = array_ops.reshape(d, [-1, attn_size])

    return ds


def _local_attention(decoder_hidden_state, hidden_features, va, hidden_attn, attention_vec_size,
                     attn_length, attn_size, batch_size, window_size=10, content_function=VINYALS_KAISER,
                     last_layer_output=None, dtype=tf.float32):
    """Put attention masks on hidden using hidden_features and query.
    Parameters
    ----------
    decoder_hidden_state

    Returns
    -------

    """

    sigma = window_size / 2
    denominator = sigma ** 2

    with vs.variable_scope("AttentionLocal_%d" % 0):

        a = None
        ht = None

        if content_function is LUONG_DOT:

            assert last_layer_output is not None

            a = math_ops.reduce_sum((last_layer_output * hidden_features), [2, 3])

        elif content_function is LUONG_GENERAL:

            assert last_layer_output is not None

            ht = rnn_cell.linear([decoder_hidden_state], attention_vec_size, True)

            a = math_ops.reduce_sum((last_layer_output * hidden_features), [2, 3])

        else:

            # this code calculate the W2*dt part of the equation - we do this here because we need
            # to obtain the current hidden states from the decoder
            linear_trans = rnn_cell.linear([decoder_hidden_state, decoder_hidden_state], attention_vec_size * 2, True)
            y, ht = tf.split(1, 2, linear_trans)

            y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])
            ht = array_ops.reshape(ht, [-1, 1, 1, attention_vec_size])

            # Attention mask is a softmax of v^T * tanh(W1*h_1 + W2*decoder_hidden_state)
            # reduce_sum is representing the + of (W1*h_1 + W2*decoder_hidden_state)
            # W1*h1 = hidden_features
            # W2*dt = y
            s = math_ops.reduce_sum(va * math_ops.tanh(hidden_features + y), [2, 3])

            a = nn_ops.softmax(s)

        # get the parameters (vp)
        vp = vs.get_variable("AttnVp_%d" % 0, [attention_vec_size])

        # tanh(Wp*ht)
        tanh = math_ops.tanh(ht)
        # S * sigmoid(vp * tanh(Wp*ht))  - this is going to return a number
        # for each sentence in the batch - i.e., a tensor of shape batch x 1
        S = attn_length
        pt = math_ops.reduce_sum((vp * tanh), [2, 3])
        pt = math_ops.sigmoid(pt) * S

        # now we get only the integer part of the values
        pt = tf.floor(pt)

        # we now create a tensor containing the indices representing each position
        # of the sentence - i.e., if the sentence contain 5 tokens and batch_size is 3,
        # the resulting tensor will be:
        # [[0, 1, 2, 3, 4]
        #  [0, 1, 2, 3, 4]
        #  [0, 1, 2, 3, 4]]
        #
        indices = []
        for pos in xrange(attn_length):
            indices.append(pos)
        indices = indices * batch_size
        idx = tf.convert_to_tensor(indices, dtype=dtype)
        idx = tf.reshape(idx, [batch_size, attn_length])

        # here we calculate the boundaries of the attention window based on the ppositions
        low = pt - window_size + 1  # we add one because the floor op already generates the first position
        high = pt + window_size

        # here we check our positions against the boundaries
        mlow = tf.to_float(idx < low)
        mhigh = tf.to_float(idx > high)

        # now we combine both into a pre-mask that has 0s and 1s switched
        # i.e, at this point, True == 0 and False == 1
        m = mlow + mhigh  # batch_size

        # here we switch the 0s to 1s and the 1s to 0s
        # we correct the values so True == 1 and False == 0
        mask = tf.to_float(tf.equal(m, 0.0))

        # here we switch off all the values that fall outside the window
        # first we switch off those in the truncated normal
        masked_soft = a * mask

        # here we calculate the 'truncated normal distribution'
        numerator = -tf.pow((idx - pt), tf.convert_to_tensor(2, dtype=dtype))
        div = tf.truediv(numerator, denominator)
        e = math_ops.exp(div)  # result of the truncated normal distribution

        at = masked_soft * e

        # Now calculate the attention-weighted vector d.
        d = math_ops.reduce_sum(
                array_ops.reshape(at, [-1, attn_length, 1, 1]) * hidden_attn,
                [1, 2])
        ds = array_ops.reshape(d, [-1, attn_size])

    return ds
